\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfigure} % make it possible to include more than one captioned figure/table in a single float
\usepackage{graphicx}
\usepackage{multirow}
% Include other packages here, before hyperref.
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\cvprfinalcopy % *** Uncomment this line for the final submission
\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
%use \x, \y, and \w for bold vector forms
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\DeclareMathOperator*{\argmax}{arg\,max}
\begin{document}
%%%%%%%%% TITLE
\title{
Project in CSE 250B\\
Assignment 3: Conditional Random Fields}
\author{Andreas Landstad, Spencer Bliven, Jonas Hoelzler\\
Computer Science Department\\
University of California, San Diego\\
{\tt\small landstad.andreas@gmail.com, sbliven@ucsd.edu, jonas@hoelzler.de}
}% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
\maketitle
\thispagestyle{empty}
%%%%%%%%% ABSTRACT
\begin{abstract}
\end{abstract}
%%%%%%%%% BODY TEXT
%INDICES:
% words n=1...N
% features j=1...J
% positions i=1...M_n
% tags k=1...V
%LABELS
% y_n is the correct label
% y' is any label
% \hat{y} is the predicted label
% tags are u or v
% problem: y_1 is ambiguous: i=1 or n=1?
\section{Introduction}
Conditional random fields provide a generic model for performing supervised learning. Generic feature functions are used to transform complicated input data and labels into real numbers. This abstraction allows machine learning and statistical analysis on very complicated datasets.
One such complicated problem is the automatic detection of syllable boundaries within words. Possible inputs include all words in a given language, plus the possibility of additional  neologisms which may be coined after the training period. Developing a classifier to determine syllable boundaries is an important task for automatic hyphenation, speech synthesis, and linguistic analysis.
\subsection{Conditional Random Field model}
The goal of supervised classification is to predict a label, $\y$, from some input data, $\x$. Conditional random fields do not place any restrictions on the type of data and labels being learned. To allow this abstraction, conditional random fields introduces the concept of \begin{em}feature functions\end{em} of the form $F(\x,\y)$ which attempt to measure the compatibility of $\x$ with the label $\y$. A number of such feature functions are calculated for each $(\x,\y)$ pair, and the weighted sum of $J$ feature functions is used to determine the probability that $\y$ is the correct label for $\x$, according the the equation
\begin{equation}
        p(\y\,|\,\x;\w) = \frac{\exp \left( \sum_{j=1}^J w_j F_j (\x,\y) \right)}{z(\x;\w)}
\end{equation}
where $z(\x;\w)$ normalizes the total mass over all possible labels $\y'$ to one and is equal to
\begin{equation}
        z(\x;\w) = \sum_{\y'} \exp \left( \sum_{j=1}^J w_j F_j (\x,\y) \right). \label{Eq:partition}
\end{equation}
To use this model for inference, one simply assigns the most probable label for a given $\x$,
\begin{align}
        \hat{\y} & =\argmax_{\y} \, p(\y\,|\,\x;\w) \\
                &= \argmax_{\y} \sum_{j=1}^J w_j F_j(\x,\y).
                \label{equation}
\end{align}
The weights vector $\w$ is learned according to the principle of maximum likelihood. Thus, for some set of training pairs $(\x_n,\y_n)$ for $n=\left\{1,2,\dots,N\right\}$, the objective function is to maximize the total log likelihood w.~r.~t. $\w$ over all points of
\begin{align}
        LCL(\w) &= \sum_{n=1}^N \log\,p(\y_n\,|\,\x_n ;\,\w) \\
                &= \sum_{n=1}^N \left( \sum_{j=1}^J w_j F_j (\x_n,\y_n) - \log z(\x_n;\w) \right).
\end{align}
During training, $L_2$ regularization is used such that most weights are near zero. This is important to prevent overfitting, since the number of features is often greater than the number of training pairs, resulting in an underspecified system. The regularized objective is
\begin{align}
        LCL^*(\w) &= \sum_{n=1}^N \log\,p(\y_n\,|\,\x_n ;\,\w) - \lambda N\parallel \w \parallel^2 \\
                &= \sum_{n=1}^N \left( \sum_{j=1}^J w_j F_j (\x_n,\y_n)  - \log z(\x_n;\w) \right) \notag \\
                &\hspace{1 em}  - \lambda N\parallel \w \parallel^2
                \label{Eq:rLCL}
\end{align}
where $\lambda$ is the strength of regularization.
\subsection{Linear Chain CRFs}
Training a CRF means finding the weight vector w that gives the best possible prediction for equation (\ref{equation}) for each training example $\x$. The argmax computation is done efficiently by the Viterbi algorithm. 
The $g_i$ values are assumed to be given by the preprocessing as a $m$ by $m$ Matrix:
\begin{align}
g_i(y_{i-1},y_i) = \sum_{j=1}^{J} w_j*f_j(y_{i-1},y_{i},x,i))  
\end{align}
Let $v$ range over the range of tags. Define $U(k, v)$ to be the score of the best
sequence of tags from position $1$ to position $k$, where tag number $k$ is required to
equal $v$. This is a maximization over $k - 1$ tags because tag number k is fixed to
have value v. Formally, 
\begin{align}
U(k,v)=max_{y_1 .. y_k} \sum_{i=1}^{k-1} g_i(y_{i-1},y_i) + g_i(y_{i-1},v)
\end{align}
which can be implemented efficiently recursive as
\begin{align}
U(k, v) = max_u [U(k-1, u) + g_k(u, v)]
\label{recu}
\end{align}
Now, one can backtrack the path, which gives the highest probability.
\subsection{Training via Stochastic Gradient Ascent}
The weight vector $\w$ that maximizes Equation \ref{Eq:rLCL} can be found using gradient ascent methods. The gradient of the $LCL$ with respect to the $j$th component of $\w$ is
\begin{align}
        \frac{\partial LCL^*}{\partial \w_j } &= 
                \sum_{n=1}^N \frac{\partial}{\partial \w_j }  \log\,p(\y_n\,|\,\x_n ;\,\w)  \notag \\
        &\hspace{1 em} - \lambda N  \frac{\partial}{\partial \w_j }\parallel \w \parallel^2 \\
        %&= \sum_{n=1}^N \left( F_j(x_n,y_n) -  \sum_{\y'} p(\y'\,|\,\x_n ;\,\w)\,F_j(\x_n, \y' ) \right)  \notag \\
        %&\hspace{1 em}         - 2 \lambda N   \w_j \\
        &= \sum_{n=1}^N \left( F_j(\x_n,\y_n) -   \mathbb{E}_{\y'\,|\,\x_n} \left[ F_j(\x_n, \y' ) \right] \right)  \notag \\
        &\hspace{1 em}         - 2 \lambda N   \w_{j}. \label{Eq:gradient}
\end{align}
The expectation in equation \ref{Eq:gradient} can be calculated by summing over all possible labels $\y'$ according to
\begin{align}
        \mathbb{E}_{\y'\,|\,\x_n} \left[ F_j(\x_n, \y' ) \right] &= \sum_{\y'} p(\y'\,|\,\x_n ;\,\w)\,F_j(\x_n, \y' ). \label{Eq:expectation}
\end{align}
% Define \M for forwards-backwards score
\newcommand{\M}{\mathcal{M}}
For general CRFs computing the expectation is computationally intensive since it requires summing over $V^M_n$ possible length-$M_n$ labels. For linear chain CRFs this can be solved in time $O(M_n V^2 J^2 + V^2 M_n)$ using the 
\begin{em}forwards-backwards\end{em}
algorithm, which is large but polynomial. The forwards-backwards algorithm involves computing two functions recursively. Let $\alpha(i,u)$ be the unnormalized  probability of the tag sequence $y_1 y_2 \dots y_i$ ending with $y_i  = u$. This can be calculated as
\begin{align}
        \alpha(i,u) &= \sum_{k=1}^V \alpha(i-1, v_k) \M_i(v_k,u)
\end{align}
for all tags $v_k$, where $\M_i(v_k,u)$ is the score
\begin{align}
        \M_i(v,u) &= \exp \sum_{j=1}^J w_j\, f_j(v,u,\x,i).
\end{align}
The base case for the forwards vector is
\begin{align}
        \alpha(1,v) = \M_i(\mbox{\textsc{begin}},v)
\end{align}
The backwards vector, $\beta(u,i)$, is the unnormalized probability of the tag sequence $y_i y_{i+1}\dots y_{M}$.  This is given as
\begin{align}
        \beta(u,i) = \sum_{k=1}^V \beta(v_k,i+1)\M_{i+1}(u,v_k)
\end{align}
with base case
\begin{align}
        \beta(v,n) = \M_{i+1}(u,\mbox{\textsc{end}}).
\end{align}
Together, the forwards and backwards algorithm can be used to calculate the partition function (Eq.\ \ref{Eq:partition}) and conditional expectation (Eq.\ \ref{Eq:expectation}).
\begin{align}
        z(\x_n;\w) &= \sum_{k=1}^V \alpha(M_n, v_k) \M_{M_n}(v_k,\mbox{\textsc{end}}) \\
        &= \sum_{k=1}^V \beta(v_k, 1)  \M_1(\mbox{\textsc{begin}}, v_k)
\end{align}
\begin{align}
        p(y_i = u | \x; \w) &= \frac{\alpha(i,u)\beta(u,i)}{z(\x;\w)}
\end{align}
\begin{align}
        p(y_{i-1}=u,y_i=v|\x;\w) &= \frac{\alpha(i,u)\M_{i+1}\beta(v,i+1)}{z(\x;\w)}
\end{align}
\begin{align}
        \mathbb{E}_{\y'\,|\,\x_n} & \left[ F_j(\x_n, \y' ) \right] 
                = \sum_{i=1}^{M_n} \mathbb{E}_{\y'\,|\,\x_n} \left[f_j(y_{i-1},y_i,\x_n, i ) \right] \\
                &= \sum_{i=1}^{M_n} \sum_{k=1}^V \sum_{l=1}^V \big[ p(y_{i-1}=v_k,y_i=v_l\,|\,\x_n ;\,\w) \notag \\
                & \hspace{6 em} \cdot f_j(y_{i-1},y_i,\x_n, i ) \big] \\
                &= \sum_{i=1}^{M_n} \sum_{k=1}^V \sum_{l=1}^V \Big[ f_j(y_{i-1},y_i,\x_n, i ) \notag \\
                & \hspace{6 em} \cdot  \frac{\alpha(i,u)\M_{i+1}\beta(v,i+1)}{z(\x;\w)} \Big]
\end{align}

Using the forwards-backwards algorithm to calculate the gradient of $LCL*$, standard stochastic gradient ascent can be used to find the maximum $\w$ with the update rule
\begin{align}
	w_j \leftarrow w_j + \lambda \frac{\partial}{\partial w_j} LCL^*(\y_n|\x_n; \w)
\end{align}
for each training pair $n=1,2,\ldots,N$.


\subsection{Collin's Perceptron Algorithm}
Although the forwards-backwards algorithm allows the computation of the gradient in polynomial time, this is still a significant computational challenge. Collin’s Perceptron Algorithm attempts to improve on that time by approximating the gradient.

By placing all the probability mass on the most likely $\y$ value, i.~e.~by using the approximation $p(y|x;w) = I(y = \hat{y})$, where $\hat{y}=argmax_y p(y|x;w)$,
the stochastic gradient update rule simplifies to 
\begin{align}
        w_j := w_j + F_j(x, y)
        w_j := w_j . F_j(x, \hat{y}).
\end{align}
This rule is applied for every weight $w_j$, for a given training example $\langle x, yi\rangle$. Given
a training example $\x$, the label $\hat{y}$ can be thought of as an impostor compared to
the genuine label y \ref{elkan}.


\section{Methods}

\subsection{Zulu Dataset}
The Zulu dataset consists of 10,040 Zulu words with correct segmentations indicated. There are 107 words in the corpus that have more than one correct segmentation and are ignored, leaving 9933 words. The longest word had 24 letters, the longest syllabus consists of 16 letters. 

\subsection{Feature Selection}
\begin{em}Window-features.\end{em} We made generic features that included all different windows of three, four, five, six or seven letters that existed in the words of the dataset. These features captured the letters of this part of the word together with two \begin{em}labels\end{em} indicating whether or not there was a syllable-split between the second last (label $\y_{i-1}$) and last (label $\y_i$) letter of the window in this word. One of the three-window feature functions generated from the word $\_j$: "abcdefg" hyphenated: "ab-cde-fg", would look as follows:
\begin{align}
f_j(\bar{x},i,\y_{i-1},\y_i) =&I(\x_{i-2}=a)I(\x_{i-1}=b)I(\x_i=a)\notag\\
&I(\y_{i-1}=1)I(\y_i=0)
\end{align}
Here $\x_i$ is the letter at position i in the I(statement) is the indicator function which yields '1' if the statement is true.
\begin{em}Suffix- and prefix-features.\end{em}
In order to capture beginnings and endings we also made similar features for prefixes and suffixes. These features are windows of length one to three in the beginning and end of the word and consider the words $\bar{x}$s) and labels($\y$s)
\begin{em}Vowel- and consonant-features.\end{em}
In addition we looked at vowel- and consonant-patterns. As Linda Van Huyssteen \cite{huyssteen03} showed there is a consonant-vowel-pattern, that is, syllables tend to end with a vowel. In order to capture this we made features with consonants/vowels(cv) in two and two letters together with their labels. We thus has 16 features of the form:
\begin{align}
f_j(\bar{x},i,\y_{i-1},\y_i)=&I(\y_{i-1}=0)I(\y_i=1)I(\x_{i-1} is a consonant)I(\x_{i} is a vowel)
\end{align}

Example above is the feature capturing the consonant-vowel-hyphenation-pattern. Other patterns that were common were vowel, consonant with hyphenation between the two and patterns with consonant, vowel and no hyphenation between the two.


\subsection{Design of the Viterbi algorithm}
The Viterbi algorithm was implemented by first creating a table of score values $U(k,v)$. The table was created recursively by using Equation \ref{recu}. The score for the begin tags were added to $U(0,v)$. Before starting the backtracking, the score for the end tag were added to U(0,k).
followed by a backtracking phase.

\subsection{Design of Collins Perceptron}



\subsection{Evaluation}
%Remember to put in real numbers here
Five-fold cross validation was used to evaluate the performance of our algorithm.  In each case, ~8000 words were used to optimize parameter settings and train the feature weights. 
\section{Results}
\section{Discussion}
\section{Conclusion}
%\nocite{}
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}
\end{document}