\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfigure} % make it possible to include more than one captioned figure/table in a single float
\usepackage{graphicx}
\usepackage{multirow}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi

%use \x, \y, and \w for bold vector forms
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\DeclareMathOperator*{\argmax}{arg\,max}


\begin{document}

%%%%%%%%% TITLE
\title{
Project in CSE 250B\\
Assignment 3: Conditional Random Fields}

\author{Andreas Landstad, Spencer Bliven, Jonas Hoelzler\\
Computer Science Department\\
University of California, San Diego\\
{\tt\small landstad.andreas@gmail.com, sbliven@ucsd.edu, jonas@hoelzler.de}
}% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
\maketitle
\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Conditional random fields provide a generic model for performing supervised learning. Generic feature functions are used to transform complicated input data and labels into real numbers. This abstraction allows machine learning and statistical analysis on very complicated datasets.

One such complicated problem is the automatic detection of syllable boundaries within words. Possible inputs include all words in a given language, plus the possibility of additional  neologisms which may be coined after the training period. Developing a classifier to determine syllable boundaries is an important task for automatic hyphenation, speech synthesis, and linguistic analysis.

\subsection{Conditional Random Field model}



The goal of supervised classification is to predict a label, $\y$, from some input data, $\x$. Conditional random fields do not place any restrictions on the type of data and labels being learned. To allow this abstraction, conditional random fields introduces the concept of \begin{em}feature functions\end{em} of the form $F(\x,\y)$ which attempt to measure the compatibility of $\x$ with the label $\y$. A number of such feature functions are calculated for each $(\x,\y)$ pair, and the weighted sum of $J$ feature functions is used to determine the probability that $\y$ is the correct label for $\x$, according the the equation
\begin{equation}
	p(\y\,|\,\x;\w) = \frac{\exp \left( \sum_{j=1}^J w_j F_j (\x,\y) \right)}{z(\x;\w)}
\end{equation}
where $z(\x;\w)$ normalizes the total mass over all possible labels $\y'$ to one and is equal to
\begin{equation}
	z(\x;\w) = \sum_{\y'} \exp \left( \sum_{j=1}^J w_j F_j (\x,\y) \right).
\end{equation}

To use this model for inference, one simply assigns the most probable label for a given $\x$,
\begin{align}
	\hat{\y} & =\argmax_{\y} \, p(\y\,|\,\x;\w) \\
		&= \argmax_{\y} \sum_{j=1}^J w_j F_j(\x,\y).
\end{align}

The weights vector $\w$ is learned according to the principle of maximum likelihood. Thus, for some set of training pairs $(\x_i,\y_i)$ for $i=\left\{1,2,\dots,N\right\}$, the objective function is to maximize the total log likelihood w.~r.~t. $\w$ over all points of
\begin{align}
	LCL(\w) &= \sum_{i=1}^N \log\,p(\y_i\,|\,\x_i ;\,\w) \\
		&= \sum_{i=1}^N \left( \sum_{j=1}^J w_j F_j (\x_i,\y_i) - \log z(\x_i;\w) \right).
\end{align}
During training, $L_2$ regularization is used such that most weights are near zero. This is important to prevent overfitting, since the number of features is often greater than the number of training pairs, resulting in an underspecified system. The regularized objective is
\begin{align}
	LCL^*(\w) &= \sum_{i=1}^N \log\,p(\y_i\,|\,\x_i ;\,\w) - \lambda N\parallel \w \parallel^2 \\
		&= \sum_{i=1}^N \left( \sum_{j=1}^J w_j F_j (\x_i,\y_i)  - \log z(\x_i;\w) \right) \notag \\
		&\hspace{1 em}  - \lambda N\parallel \w \parallel^2
\end{align}
where $\lambda$ is the strength of regularization.

CRF Gradient

\subsection{Linear Chain CRFs}
Jonas: Linear Chain CRFs \& connection to Viterbi algorithm
\subsection{Collin's Perceptron Algorithm}
Jonas
\subsection{--Other Method--}
Spencer

\section{Methods}
\subsection{Zulu Dataset}
Details about dataset

\subsection{Feature Selection}
Window-features:
We made generic features that included all different windows of three, four, five, six or seven letters that existed in the words of the dataset together with two _labels_ indicating whether or not there was a syllable-split between the second last (_label y-1_) and last (_label y_) letter of the window. The labels for each of the letters in the word are indicating whether or not there will be a syllable split between this letter and the next one. One three window feature generated from word j: "abcdefg" splitted as follows: "ab-cde-fg" would look as follows:

fj(xbar,i,y-1,y)=I(xi-2=a)*I(xi-1=b),I(xi=a)I(y-1=1)*I(y=0)

Here xi is the letter at position i in the I(statement) is the indicator function which yields '1' if the statement between the paranthesis is true.


Two encodings: 001 vs. 123

Features used for each encoding

\subsection{Evaluation}
%Remember to put in real numbers here
Five-fold cross validation was used to evaluate the performance of our algorithm.  In each case, ~8000 words were used to optimize parameter settings and train the feature weights. 



\section{Results}
\section{Discussion}
\section{Conclusion}


%\nocite{}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
