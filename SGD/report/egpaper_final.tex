\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfigure} % make it possible to include more than one captioned figure/table in a single float
\usepackage{graphicx}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{
Project in CSE 250B\\
Assignment 2: Logistic Regression with Regularization}

\author{Andreas Landstad, Spencer Bliven, Jonas Hoelzler\\
Computer Science Department\\
University of California, San Diego\\
{\tt\small landstad.andreas@gmail.com, sbliven@ucsd.edu, jonas@hoelzler.de}
}% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
\maketitle
\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
This project applies data mining methods to analyze a dataset describing two email campaigns and a control group. The datasets is analyzed using a logistic regression and real-valued regression training method with stochastic gradient descent as optimization method.
The results show, that future mails should be sent to ???
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Introduction
\subsection{Logistic Regression}

\renewcommand{\b}{\boldsymbol{\beta}}

Logistic regression is a common model for binary classification \cite{bishop06}. Given an example $x\in\mathbb{R}^d$, the probability of a label $y\in\{0,1\}$ is given by the logistic sigmoid function
\[p(Y=1\ |\ X=\mathbf{x};\ \b) = \frac{1}{1+\exp(-\b\cdot \mathbf{x})} \]
where $\b$ is a $d$-dimensional vector. We assume $x_0=1$ such that $\b_0$ acts as an intercept.
Given a set of parameters $\b$, the logistic function can be used for classification:
\[ \hat{y} = \operatorname*{arg\,max}_y\,p(Y=y\ |\ X=\mathbf{x};\ \b) \]

To learn useful parameters $\b$ we utilize a training set $(X,Y) = \left\{ (\mathbf{x}_i,y_i)\ |\ i=1\dots N \right\}$. The optimal $\b^*$ is chosen such that it maximizes the regularized log conditional likelihood over the training set
\begin{align*}
\b^* &= \operatorname*{arg\,max}_{\b}\,LCL(\b;\ Y|X) - \alpha \, \parallel \b \parallel^2 \\
&= \operatorname*{arg\,max}_{\b}\,\sum_{i=1}^{N} \log p(y_i\ |\ \mathbf{x}_i;\ \b) - \alpha \, \parallel \b \parallel^2 .
\end{align*}
The optimal $\b^*$ is found numerically using stochastic gradient descent (SGD). An initial $\b_0 = \mathbf{0}$ is chosen. For each example in the training set, $\b$ is updated according to the rule
\begin{align*}
\b_{t+1} &= \b_{t} + \lambda \, \nabla  \left( LCL(\b;\ Y|X) - \alpha \, \parallel  \b \parallel^2 \right) \\
&= \b_{t} + \lambda \,\left( (y_i-p_i)\, \mathbf{x}-2\alpha \b \right)
\end{align*}

\subsection{Design of Experiments}

\subsection{Description and encoding of the dataset}
The learning algorithms are applied to the dataset of Hillstrom. The data is given as comma separated file.
For the learning algorithm, the rows of the dataset are encoded in 29-dimensional $x$-vectors.
Hillstrom collected historical customer attributes of the customers.
First, the recency entry shows the number of months since last purchase from one to twelve months. In this project it is encoded as a bit-vector of eleven bits (bit 1 to 11), where the $i$th bit is $1$ for a recency of $i$ and $0$ otherwise. The $12$th bit is left out, since it only represents redundant information.
The history segmentation is a categorization of the customers in 7 categories, using how many dollars were spent in the last year.
(TODO: Dollars in categories). It is encoded again as bit-vector 12 to 17, while one bit is again left out due to redundancy.
Entry 18 shows the real value of actual dollars spent in the past year. Entry 19 and 20 are true, if the customer purchased Mens respectively  Womens merchandise in the past year. Bit 21 and 22 categorize the customer in its zip code area. Bit 21 is $1$, if he is from a rural area, bit 22 is $1$, if he is from a urban area, the bit for the suburban area is again left out.
Bit 23 is the indicator, if the customer is a new customer mentioned the last twelve months.
The channel describes the channel the customer purchased  from  in the past year. Bit 24 is $1$ for the phone, bit 25 is $1$ for the web. The bits are not mutually exclusive. 

Segment
Visit
Conversion
Spend

\begin{itemize}
\item 1-11 bits recency (Real 1-11, months since last purchase)  (Month 12 left out)
\item 12-17 bits history segment (1-6) (0/1, 1 = made a purchase in category 1-7)   (Category 7 left out)
\item  18 value history (Real positive, amount spent in past year)
\item  19 bit mens (0/1, 1= purchased mens merchandise in past year)
\item  20 bit womens (0/1, 1= purchased womens merchandise in past year)
\item  21-22 bit zip code (rural,urban) (0/1, 1 if purchased in a rural, suburban, or urban zipcode)  (Suburban left out)
\item  23 bit newbie (0/1, 1= new customer)
\item  24-25 bit channel (phone,web,multi) (0/1, 1= purchased from phone, web, or both 1 if multi channels)
\item 26 bit visit
\item 27 bit conversion
\item 28 value spend
\end{itemize}

Moreover an intercept $x_0$ is added.
Because the learning rate is the same for every parameter, it is useful to scale the features $x_j$ so that their magnitudes are similar for all $j$. Given that the intercept $x_0$ has constant value 1, the other features are normalized having mean zero and variance 1(\cite{elkan11}).

\begin{figure*}[tb]
	\centering
 	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  	\hline
  	 0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17\\ 
  	 \hline
  	 Intercept&\multicolumn{11}{|c|}{Recency}&\multicolumn{6}{|c|}{History segment}\\
  	 \hline
		I&1&2&3&4&5&6&7&8&9&10&11&1&2&3&4&5&6\\ 
  	\hline
 		\end{tabular}
 		


 		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
  	\hline
  	 18&19&20&21&22&23&24&25&26&27&28\\ 
  	 \hline
  	 History&Mens&Womens&\multicolumn{2}{|c|}{Zip-Code}&Newbie&\multicolumn{2}{|c|}{Channel}&Visit&Conversion&Spent\\
  	 \hline
		H&M&W&U&R&N&P&W&V&C&S\\ 
  	\hline
 		\end{tabular}
\caption{Encoding}
    \label{encoding}
\end{figure*}

\section{Complexity Analysis}

\section{Implementation}


\section{Evaluation}



%------------------------------------------------------------------------

\section{Conclusions}


\nocite{hillstrom08,bottou11,elkan11}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
